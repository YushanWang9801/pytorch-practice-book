# ç¬¬ 3 ç« ï¼šæ¿€æ´»å‡½æ•°ä¸æŸå¤±å‡½æ•°ï¼ˆActivation & Lossï¼‰

åœ¨æ„å»ºç¥ç»ç½‘ç»œæ—¶ï¼Œ**æ¿€æ´»å‡½æ•°** å†³å®šäº†æ¨¡å‹çš„éçº¿æ€§èƒ½åŠ›ï¼Œ**æŸå¤±å‡½æ•°** åˆ™å®šä¹‰äº†æ¨¡å‹çš„ä¼˜åŒ–ç›®æ ‡ã€‚

---

## ğŸ”‹ 1. æ¿€æ´»å‡½æ•°ï¼ˆActivation Functionsï¼‰

### ä¸ºä»€ä¹ˆéœ€è¦æ¿€æ´»å‡½æ•°ï¼Ÿ

æ²¡æœ‰æ¿€æ´»å‡½æ•°çš„ç¥ç»ç½‘ç»œæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªçº¿æ€§å˜æ¢ï¼š
\[
y = W_2(W_1x + b_1) + b_2
\]
åŠ å†å¤šå±‚éƒ½æ˜¯çº¿æ€§çš„ç»„åˆã€‚è€Œæ¿€æ´»å‡½æ•°å¼•å…¥**éçº¿æ€§å˜æ¢**ï¼Œè®©ç½‘ç»œå…·å¤‡æ‹Ÿåˆå¤æ‚å‡½æ•°çš„èƒ½åŠ›ã€‚

---

### âœ… å¸¸è§æ¿€æ´»å‡½æ•°åŠå…¶æ€§è´¨

| å‡½æ•° | å…¬å¼ | èŒƒå›´ | å¯¼æ•°æ˜¯å¦è¿ç»­ | æ˜¯å¦é¥±å’Œ | åº”ç”¨ |
|------|------|------|---------------|-----------|------|
| ReLU | \( \max(0, x) \) | \([0, \infty)\) | ä¸è¿ç»­ | å¦ | é»˜è®¤æ¨è |
| Leaky ReLU | \( \max(0.01x, x) \) | \((-\infty, \infty)\) | è¿ç»­ | å¦ | é¿å… ReLU æ­»äº¡ |
| Sigmoid | \( \frac{1}{1 + e^{-x}} \) | \((0,1)\) | è¿ç»­ | æ˜¯ | è¾“å‡ºä¸ºæ¦‚ç‡ |
| Tanh | \( \tanh(x) \) | \((-1, 1)\) | è¿ç»­ | æ˜¯ | é›¶ä¸­å¿ƒ |
| GELU | è¿‘ä¼¼ \( x \Phi(x) \) | ç±»ä¼¼ ReLU | è¿ç»­ | å¦ | Transformer é»˜è®¤ |

---

### ğŸ“¦ PyTorch ä¸­çš„æ¿€æ´»å‡½æ•°è°ƒç”¨æ–¹å¼

#### 1. ä½œä¸ºæ¨¡å—ï¼ˆç”¨äº `nn.Sequential`ï¼‰
```python
import torch.nn as nn

nn.ReLU()
nn.Tanh()
nn.Sigmoid()
nn.GELU()
```

#### 2. ä½œä¸ºå‡½æ•°ï¼ˆç”¨äºå‡½æ•°å¼ APIï¼‰

```python
import torch.nn.functional as F

F.relu(x)
F.sigmoid(x)
F.tanh(x)
F.gelu(x)
```

---

### ğŸ§® å„æ¿€æ´»å‡½æ•°çš„å¯¼æ•°ï¼ˆè‡ªåŠ¨æ±‚å¯¼ï¼‰

```python
import torch
x = torch.tensor([1.0], requires_grad=True)
y = torch.relu(x)
y.backward()
print(x.grad)  # 1.0
```

ä½ æ— éœ€æ‰‹åŠ¨å†™å¯¼æ•°ï¼ŒPyTorch ä¼šè‡ªåŠ¨æ ¹æ®è®¡ç®—å›¾åå‘ä¼ æ’­ã€‚

---

## ğŸ“‰ 2. æŸå¤±å‡½æ•°ï¼ˆLoss Functionsï¼‰

### æŸå¤±å‡½æ•°çš„ä½œç”¨ï¼Ÿ

å®šä¹‰äº†æ¨¡å‹è¾“å‡º `Å·` ä¸çœŸå®æ ‡ç­¾ `y` ä¹‹é—´çš„â€œè·ç¦»â€ã€‚ä¼˜åŒ–å™¨ä¼šå°è¯•æœ€å°åŒ–è¿™ä¸ªè·ç¦»ã€‚

---

### âœ… å¸¸è§æŸå¤±å‡½æ•°ä¸€è§ˆ

#### 1. MSE Lossï¼ˆå‡æ–¹è¯¯å·®ï¼‰

ç”¨äºå›å½’ä»»åŠ¡ï¼š

$$
\text{MSE}(y, \hat{y}) = \frac{1}{N} \sum (y_i - \hat{y}_i)^2
$$

```python
loss = nn.MSELoss()
```

#### 2. L1 Lossï¼ˆç»å¯¹å€¼è¯¯å·®ï¼‰

$$
\text{L1}(y, \hat{y}) = \frac{1}{N} \sum |y_i - \hat{y}_i|
$$

```python
loss = nn.L1Loss()
```

#### 3. CrossEntropy Lossï¼ˆäº¤å‰ç†µï¼‰

ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œç­‰ä»·äºï¼š

```python
nn.CrossEntropyLoss() == nn.LogSoftmax + nn.NLLLoss
```

è¾“å…¥è¦æ±‚ï¼š

* `input`: shape = (N, C) â†’ logitsï¼ˆæœªç»è¿‡ softmax çš„è¾“å‡ºï¼‰
* `target`: shape = (N,) â†’ ç±»åˆ«ç´¢å¼•ï¼ˆå¦‚ `0`, `1`, `2`ï¼‰

```python
loss = nn.CrossEntropyLoss()
output = model(x)      # shape = (batch_size, num_classes)
loss_val = loss(output, target)
```

#### 4. BCE Lossï¼ˆäºŒåˆ†ç±»ï¼‰

ç”¨äºè¾“å‡ºä¸ºæ¦‚ç‡çš„äºŒåˆ†ç±»ä»»åŠ¡ï¼ˆè¾“å…¥éœ€ç»è¿‡ sigmoidï¼‰

```python
loss = nn.BCELoss()
F.sigmoid(output) â†’ loss(...)
```

æˆ–æ›´æ¨èä½¿ç”¨ï¼š

```python
loss = nn.BCEWithLogitsLoss()  # å†…éƒ¨å·²åŒ…å« sigmoid
loss(output, target)
```

---

### ğŸ“Š ç¤ºä¾‹ï¼šè®¡ç®—å¹¶åå‘ä¼ æ’­æŸå¤±

```python
import torch
import torch.nn as nn

y_pred = torch.tensor([[0.3], [0.7]], requires_grad=True)
y_true = torch.tensor([[0.0], [1.0]])

loss_fn = nn.BCELoss()
loss = loss_fn(y_pred, y_true)
loss.backward()

print(loss.item())
```

---

## ğŸ¤” æ€»ç»“ï¼šå¦‚ä½•é€‰æ‹©æ¿€æ´»å‡½æ•°å’ŒæŸå¤±å‡½æ•°ï¼Ÿ

| ä»»åŠ¡ç±»å‹ | è¾“å‡ºå±‚æ¿€æ´»                           | æŸå¤±å‡½æ•°                 |
| ---- | ------------------------------- | -------------------- |
| å›å½’   | æ—  / Linear                      | `MSELoss` / `L1Loss` |
| äºŒåˆ†ç±»  | `Sigmoid` æˆ– `BCEWithLogitsLoss` | `BCEWithLogitsLoss`  |
| å¤šåˆ†ç±»  | `Softmax`ï¼ˆç”± CrossEntropy å†…éƒ¨å¤„ç†ï¼‰  | `CrossEntropyLoss`   |

---

## ğŸ›  è®­ç»ƒç¤ºæ„å›¾ï¼ˆä»¥äºŒåˆ†ç±»ä¸ºä¾‹ï¼‰

```python
model = nn.Sequential(
    nn.Linear(10, 1),
    nn.Sigmoid()
)
loss_fn = nn.BCELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

for x, y in dataloader:
    pred = model(x)
    loss = loss_fn(pred, y)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
```

---

## âœ… å°ç»“

| åˆ†ç±»   | å‡½æ•°                                           | è¯´æ˜             |
| ---- | -------------------------------------------- | -------------- |
| æ¿€æ´»å‡½æ•° | `ReLU`, `Sigmoid`, `Tanh`, `GELU`            | æ§åˆ¶éçº¿æ€§èƒ½åŠ›        |
| æŸå¤±å‡½æ•° | `MSE`, `L1`, `CrossEntropy`, `BCEWithLogits` | è¡¡é‡æ¨¡å‹å¥½å         |
| è‡ªåŠ¨æ±‚å¯¼ | `.backward()`                                | è‡ªåŠ¨é“¾å¼æ±‚å¯¼ï¼Œæ— éœ€æ˜¾å¼å†™æ¢¯åº¦ |